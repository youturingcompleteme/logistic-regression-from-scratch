{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import + test/train sets to NumPy Arrays\n",
    "\"\"\"\n",
    "# Import libraries & data - Set up training/test set ndarrays\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_train = pd.read_csv(\"/Users/FATMac/Downloads/titanic_train.csv\")\n",
    "df_test = pd.read_csv(\"/Users/FATMac/Downloads/titanic_test.csv\")\n",
    "\n",
    "train_set = df_train.values\n",
    "test_set = df_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data Wrangling\n",
    "Data should be reshaped as m x n design matrix.\n",
    "Examples are row vectors (qty m)\n",
    "Features are column vectors (qty N = n - 1)\n",
    "\"\"\"\n",
    "# Extract single feature examples (\"Pclass\") \n",
    "train = train_set[:,[2, 5, 1]].astype(float)\n",
    "test = test_set[:,[1, 4]].astype(float)\n",
    "\n",
    "#if(train.ndim == 1):\n",
    "#    train = train[:, np.newaxis]\n",
    "\n",
    "# Mask out examples (rows) w/ NaN's in training set\n",
    "train = train[~np.isnan(train).any(axis=1)]\n",
    "test = test[~np.isnan(test).any(axis=1)]\n",
    "\n",
    "# Prepend constant offset term to X for model intercept\n",
    "train = np.insert(train, 0, 1, axis=1)\n",
    "test = np.insert(test, 0, 1, axis=1)\n",
    "\n",
    "X = train[:, 0:3] # slice feature values (first n - 1 columns)\n",
    "y = train[:, -1] # slice out labels (last column)\n",
    "X_test = test\n",
    "\n",
    "# Feature scaling - Every column gets scaled\n",
    "N = X.shape[1]\n",
    "for i in range(N):\n",
    "    X[:, i] = (X[:, i] - np.mean(X[:, i])) / np.abs(np.max(X[:, i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- Model definitions -\n",
    "In array shape comments, m is num examples; n is num features + 1\n",
    "\"\"\"\n",
    "# A Numerically Stable Sigmoid\n",
    "# \"Z\": ndarray shape (m, )\n",
    "sigmoid = lambda Z : np.where(Z > 0., \n",
    "                              1. / (1. + np.exp(-Z)), \n",
    "                              np.exp(Z) / (1. + np.exp(Z)))\n",
    "\n",
    "# Hypothesis: Decision boundary is a linear combination of feature\n",
    "# values w/ learnable intercept, squished by sigmoid...\n",
    "# \"theta\": ndarray shape (n, )\n",
    "# \"X\": ndarray shape (m, n)\n",
    "def h(theta, X):\n",
    "    return sigmoid(X.dot(theta))\n",
    "\n",
    "# Objective function to Optimize (not called): \n",
    "#     log-likelihood is convex ensuring convergence\n",
    "#     L2 Regularized (avoid overfitting w/ large weights)\n",
    "#\n",
    "# \"X\": input feature values ndarray shape (m, n)\n",
    "# \"Y\": observed labels from training data ndarray shape (m, )\n",
    "# \"theta\": model parameter ndarray shape (m, n)\n",
    "def J(X, Y, theta, tune):\n",
    "    # p = degree of belief that our model assigns to positive class\n",
    "    p = h(theta, X)\n",
    "    # Unpack size of batch\n",
    "    m, = Y.shape\n",
    "    \n",
    "    # Regularizer term\n",
    "    L2_reg = (tune/(2.*m)) * np.dot(theta, theta)\n",
    "    # Standard logistic regression cost function\n",
    "    log_likelihood = np.sum(np.multiply(Y, np.log(p)) + (1. - Y) * np.log(1. - p)) / m\n",
    "    \n",
    "    # return cost over mini-batch        \n",
    "    return (-log_likelihood + L2_reg)\n",
    "\n",
    "def SGD_step(X_mini, y_mini, theta, alpha, tune):\n",
    "    # Unpack size of mini batch\n",
    "    m, = y_mini.shape\n",
    "    \n",
    "    # Calculate gradient using mini-batch\n",
    "    err = h(theta, X_mini) - y_mini\n",
    "    grad_J = (np.dot(X_mini.T, err) + tune * theta) / m\n",
    "    \n",
    "    # Compute theta_i+1\n",
    "    theta_next = theta - alpha * grad_J\n",
    "    theta_next[0] = theta_next[0] - alpha * np.mean(err)\n",
    "    \n",
    "    # return theta_i+1 and new cost (for convergence monitoring)\n",
    "    return (theta_next, J(X_mini, y_mini, theta, tune))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][-0.42390604] [[-3.13835483 -2.41928263]]\n",
      "Accuracy from sk-learn: 0.7016806722689075\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Data Exploration & Feature Engineering\n",
    "\"\"\"\n",
    "# Check for class imbalance\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(fit_intercept=True, verbose=10)\n",
    "clf.fit(X[:, 1:3], y)\n",
    "\n",
    "print(clf.intercept_, clf.coef_)\n",
    "print ('Accuracy from sk-learn: {0}'.format(clf.score(X[:, 1:3], y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost  1 :  0.6931471805599453\n",
      "cost  10001 :  0.599210815432797\n",
      "cost  20001 :  0.5992060012730206\n",
      "cost  30001 :  0.5992060003678475\n",
      "cost  40001 :  0.599206000367676\n",
      "cost  50001 :  0.5992060003676759\n",
      "cost  60001 :  0.599206000367676\n",
      "cost  70001 :  0.5992060003676759\n",
      "cost  80001 :  0.5992060003676759\n",
      "cost  90001 :  0.5992060003676759\n",
      "cost  100001 :  0.5992060003676759\n",
      "cost  110001 :  0.5992060003676759\n",
      "cost  120001 :  0.5992060003676759\n",
      "cost  130001 :  0.5992060003676759\n",
      "cost  140001 :  0.5992060003676759\n",
      "cost  150001 :  0.5992060003676759\n",
      "cost  160001 :  0.5992060003676759\n",
      "cost  170001 :  0.5992060003676759\n",
      "cost  180001 :  0.5992060003676759\n",
      "cost  190001 :  0.5992060003676759\n",
      "cost  200001 :  0.5992060003676759\n",
      "cost  210001 :  0.5992060003676759\n",
      "cost  220001 :  0.5992060003676759\n",
      "cost  230001 :  0.5992060003676759\n",
      "cost  240001 :  0.5992060003676759\n",
      "cost  250001 :  0.5992060003676759\n",
      "cost  260001 :  0.5992060003676759\n",
      "cost  270001 :  0.5992060003676759\n",
      "cost  280001 :  0.5992060003676759\n",
      "cost  290001 :  0.5992060003676759\n",
      "coefficients:  [-2695.83275358    -3.68001795    -3.25542819]\n",
      "prediction accuracy:  0.6876750700280112\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Model Training and Evaluation\n",
    "\"\"\"\n",
    "# Hyperparameters of learning algorithm\n",
    "alpha = 0.1 # learning rate\n",
    "\n",
    "epochs = 300000 # num passes through full training data\n",
    "iters_per_epoch = 1 # ratio of batch size to mini-batch size\n",
    "\n",
    "tune = 0. # L2 regularization coefficient for overfitting control\n",
    "\n",
    "# This yields 50/50 uncertainty in sigmoid on y\n",
    "theta = np.zeros(shape=X.shape[-1], dtype=np.float64)\n",
    "\n",
    "\n",
    "# Optimization Loop\n",
    "for step in range(epochs):\n",
    "    for iterate in range(iters_per_epoch):\n",
    "        # mini-batch = full batch (non-stochastic)\n",
    "        theta, cost = SGD_step(X, y, theta, alpha, tune)\n",
    "        if step % 10000 == 0:\n",
    "            print(\"cost \", step + 1, \": \", cost)\n",
    "\n",
    "print(\"coefficients: \", theta)\n",
    "y_hat = np.round(h(theta, X))\n",
    "print(\"prediction accuracy: \", 1. - np.sum(np.abs(y_hat - y)/y.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#!/bin/python3\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "# Complete the findNumber function below.\n",
    "def findNumber(arr, k):\n",
    "    if k in arr:\n",
    "        return \"YES\"\n",
    "    else:\n",
    "        return \"NO\"\n",
    " \n",
    "            \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    fptr = open(os.environ['OUTPUT_PATH'], 'w')\n",
    "\n",
    "    arr_count = int(input().strip())\n",
    "\n",
    "    arr = []\n",
    "\n",
    "    for _ in range(arr_count):\n",
    "        arr_item = int(input().strip())\n",
    "        arr.append(arr_item)\n",
    "\n",
    "    k = int(input().strip())\n",
    "\n",
    "    res = findNumber(arr, k)\n",
    "\n",
    "    fptr.write(res + '\\n')\n",
    "\n",
    "    fptr.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2.]] [5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5.]\n",
      "[10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.\n",
      " 10. 10.]\n"
     ]
    }
   ],
   "source": [
    "id = np.identity(20) * 2\n",
    "vec = np.ones(20) * 5\n",
    "\n",
    "print (id, vec)\n",
    "print(np.matmul(vec, id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
