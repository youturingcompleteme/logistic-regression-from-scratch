{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import + test/train sets to NumPy Arrays\n",
    "\"\"\"\n",
    "# Import libraries & data - Set up training/test set ndarrays\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# String of path to training and test csv's \n",
    "train_data_location = \"/Users/FATMac/Downloads/titanic_train.csv\"\n",
    "test_data_location = \"/Users/FATMac/Downloads/titanic_test.csv\"\n",
    "\n",
    "df_train = pd.read_csv(train_data_location)\n",
    "df_test = pd.read_csv(test_data_location)\n",
    "\n",
    "train_set = df_train.values\n",
    "test_set = df_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data Wrangling\n",
    "Data should be reshaped as m x n design matrix.\n",
    "Examples are row vectors (qty m)\n",
    "Features are column vectors (qty N = n - 1)\n",
    "\"\"\"\n",
    "# Extract single feature examples (\"Pclass\") \n",
    "train = train_set[:,[2, 5, 1]].astype(float)\n",
    "test = test_set[:,[1, 4]].astype(float)\n",
    "\n",
    "#if(train.ndim == 1):\n",
    "#    train = train[:, np.newaxis]\n",
    "\n",
    "# Mask out examples (rows) w/ NaN's in training set\n",
    "train = train[~np.isnan(train).any(axis=1)]\n",
    "test = test[~np.isnan(test).any(axis=1)]\n",
    "\n",
    "# Prepend constant offset term to X for model intercept\n",
    "train = np.insert(train, 0, 1, axis=1)\n",
    "test = np.insert(test, 0, 1, axis=1)\n",
    "\n",
    "X = train[:, 0:3] # slice feature values (first n - 1 columns)\n",
    "y = train[:, -1] # slice out labels (last column)\n",
    "X_test = test\n",
    "\n",
    "# Feature scaling - Every column gets scaled\n",
    "N = X.shape[1]\n",
    "for i in range(N):\n",
    "    X[:, i] = (X[:, i] - np.mean(X[:, i])) / np.abs(np.max(X[:, i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- Model definitions -\n",
    "In array shape comments, m is num examples; n is num features + 1\n",
    "\"\"\"\n",
    "# A Numerically Stable Sigmoid\n",
    "# \"Z\": ndarray shape (m, )\n",
    "sigmoid = lambda Z : np.where(Z > 0., \n",
    "                              1. / (1. + np.exp(-Z)), \n",
    "                              np.exp(Z) / (1. + np.exp(Z)))\n",
    "\n",
    "# Hypothesis: Decision boundary is a linear combination of feature\n",
    "# values w/ learnable intercept, squished by sigmoid...\n",
    "# \"theta\": ndarray shape (n, )\n",
    "# \"X\": ndarray shape (m, n)\n",
    "def h(theta, X):\n",
    "    return sigmoid(X.dot(theta))\n",
    "\n",
    "# Objective function to Optimize (not called): \n",
    "#     log-likelihood is convex ensuring convergence\n",
    "#     L2 Regularized (avoid overfitting w/ large weights)\n",
    "#\n",
    "# \"X\": input feature values ndarray shape (m, n)\n",
    "# \"Y\": observed labels from training data ndarray shape (m, )\n",
    "# \"theta\": model parameter ndarray shape (m, n)\n",
    "def J(X, Y, theta, tune):\n",
    "    # p = degree of belief that our model assigns to positive class\n",
    "    p = h(theta, X)\n",
    "    # Unpack size of batch\n",
    "    m, = Y.shape\n",
    "    \n",
    "    # Regularizer term\n",
    "    L2_reg = (tune/(2.*m)) * np.dot(theta, theta)\n",
    "    # Standard logistic regression cost function\n",
    "    log_likelihood = np.sum(np.multiply(Y, np.log(p)) + (1. - Y) * np.log(1. - p)) / m\n",
    "    \n",
    "    # return cost over mini-batch        \n",
    "    return (-log_likelihood + L2_reg)\n",
    "\n",
    "def SGD_step(X_mini, y_mini, theta, alpha, tune):\n",
    "    # Unpack size of mini batch\n",
    "    m, = y_mini.shape\n",
    "    \n",
    "    # Calculate gradient using mini-batch\n",
    "    err = h(theta, X_mini) - y_mini\n",
    "    grad_J = (np.dot(X_mini.T, err) + tune * theta) / m\n",
    "    \n",
    "    # Compute theta_i+1\n",
    "    theta_next = theta - alpha * grad_J\n",
    "    theta_next[0] = theta_next[0] - alpha * np.mean(err)\n",
    "    \n",
    "    # return theta_i+1 and new cost (for convergence monitoring)\n",
    "    return (theta_next, J(X_mini, y_mini, theta, tune))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][-0.42390604] [[-3.13835483 -2.41928263]]\n",
      "Accuracy from sk-learn: 0.7016806722689075\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "sklearn baseline for benchmark\n",
    "\"\"\"\n",
    "# Check for class imbalance\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(fit_intercept=True, verbose=10)\n",
    "clf.fit(X[:, 1:3], y)\n",
    "\n",
    "print(clf.intercept_, clf.coef_)\n",
    "print ('Accuracy from sk-learn: {0}'.format(clf.score(X[:, 1:3], y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost  0 :  0.6931471805599453\n",
      "cost  10 :  0.6907093420560281\n",
      "cost  20 :  0.6883574260717246\n",
      "cost  30 :  0.6860880479973973\n",
      "cost  40 :  0.6838979524722454\n",
      "cost  50 :  0.6817840096609927\n",
      "cost  60 :  0.679743211487378\n",
      "cost  70 :  0.6777726678458273\n",
      "cost  80 :  0.6758696028102446\n",
      "cost  90 :  0.674031350856588\n",
      "cost  100 :  0.6722553531138099\n",
      "cost  110 :  0.6705391536558082\n",
      "cost  120 :  0.668880395845282\n",
      "cost  130 :  0.6672768187387758\n",
      "cost  140 :  0.6657262535607334\n",
      "cost  150 :  0.6642266202530787\n",
      "cost  160 :  0.662775924105637\n",
      "cost  170 :  0.6613722524716603\n",
      "cost  180 :  0.6600137715717588\n",
      "cost  190 :  0.6586987233887017\n",
      "cost  200 :  0.6574254226548006\n",
      "cost  210 :  0.6561922539329229\n",
      "cost  220 :  0.6549976687916136\n",
      "cost  230 :  0.6538401830742905\n",
      "cost  240 :  0.6527183742620456\n",
      "cost  250 :  0.6516308789292033\n",
      "cost  260 :  0.6505763902904711\n",
      "cost  270 :  0.6495536558382378\n",
      "cost  280 :  0.6485614750683516\n",
      "cost  290 :  0.6475986972925188\n",
      "coefficients:  [-2.81287135 -1.13753475 -0.26259648]\n",
      "prediction accuracy:  0.669467787114846\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Model Training and Evaluation\n",
    "\"\"\"\n",
    "# Hyperparameters of learning algorithm\n",
    "alpha = 0.1 # learning rate\n",
    "\n",
    "epochs = 300 # num passes through full training data\n",
    "iters_per_epoch = 1 # ratio of batch size to mini-batch size\n",
    "\n",
    "tune = 0. # L2 regularization coefficient for overfitting control\n",
    "\n",
    "# This yields 50/50 uncertainty in sigmoid on y\n",
    "theta = np.zeros(shape=X.shape[-1], dtype=np.float64)\n",
    "\n",
    "\n",
    "# Optimization Loop\n",
    "for step in range(epochs):\n",
    "    for iterate in range(iters_per_epoch):\n",
    "        # mini-batch = full batch (non-stochastic)\n",
    "        theta, cost = SGD_step(X, y, theta, alpha, tune)\n",
    "        if step % 10 == 0:\n",
    "            print(\"cost \", step, \": \", cost)\n",
    "\n",
    "print(\"coefficients: \", theta)\n",
    "y_hat = np.round(h(theta, X))\n",
    "print(\"prediction accuracy: \", 1. - np.sum(np.abs(y_hat - y)/y.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
